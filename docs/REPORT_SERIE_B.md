ğŸ“˜ SÃ©rie B â€” Alinhamento, Narrativa e Estabilidade Cognitiva
O que Ã©:
A SÃ©rie B investiga alinhamento como fenÃ´meno operacional, nÃ£o como acesso Ã  verdade.
Os testes analisam agentes (humanos, LLMs ou sistemas genÃ©ricos) sob ruÃ­do, memÃ³ria, narrativa adversarial e mudanÃ§a de tarefas.
Pergunta central:
Alinhamento produz verdade compartilhada ou apenas estabilidade funcional?
Testes incluÃ­dos
Local: experiments/Serie_B/
B1 â€” LLM Alignment Experiment
Analisa estabilidade interna vs divergÃªncia entre agentes rÃ­gidos, elÃ¡sticos e adversariais.
B2 â€” Task Shift
Mede perda de alinhamento quando a tarefa muda sem aviso.
B3 â€” Custo do Alinhamento
Mostra que alinhamento excessivo aumenta instabilidade sob pressÃ£o externa.
B4 â€” Over-Alignment
Demonstra rigidez cognitiva e reduÃ§Ã£o de adaptaÃ§Ã£o.
B5 â€” Sem Verdade Global
Agentes estÃ¡veis internamente divergem fortemente entre si.
B6 â€” Falha Silenciosa
Alinhamento mascara erro externo sem gerar alerta interno.
ğŸ“Œ Script consolidado:
Copiar cÃ³digo

cmr_B2_to_B6_alignment_chain.py
Resultado central da SÃ©rie B
Alinhamento â‰  verdade
Alinhamento = estabilidade sob ruÃ­do
Sistemas alinhados podem falhar coletivamente de forma silenciosa
Consenso narrativo nÃ£o corrige erro de ancoragem
